=== Requirements:

* At least one KVM host with enough memory and CPU capacity to support the deployed cluster nodes
* Each KVM host must have a linux bridge that has access to a LAN
** The LAN can be non-routable, if so desired
* An RMT server, with the CaaSP registration code and mirrored repositories, that is accessible from each KVM server
* Each KVM host should have access the Internet or a local container and Helm registry server

=== Operation:

IMPORTANT: This methodology requires that the KVM servers have a specific naming convention. It requires that they all have the same prefix, i.e. "infra" and then use a numbered suffix, ideally in sequential order. The testing environment this project was developed on had four KVM hosts named infra1, infra2, infra3, infra4. There is a small amount of flexibility in the naming scheme but any deviation from this scheme requires altering the variables.tf file to match.

IMPORTANT: This project is based on the SLES JeOS OpenStack Cloud image, from https://download.suse.com/index.jsp, which has cloud init pre-configured and ready for first boot.

* Download the latest SLES JeOS OpenStack qcow2 image into the terraform subdirectory (below where this README exists)
** If possible, choose the most recent QU* (Quarterly Update) version of the image
* Update the terraform/variables.tf file to suit your environment
** Depending on how closely your KVM hosts match the expected naming convention, the minimum changes to the variables.tf file that are required might be the number of master nodes, worker nodes, and the amount of CPU and memory resources allocated to each node.

TIP: Review the cloud init files in the cloud-init and global-cluster-cloud-init subdirectories to ensure they are performing the actions you are expecting. Some of the registration and software install/update functions may be commented out to speed deployment (which requires that the qcow2 image be registered/updated/configured ahead of time.) When in doubt, cloud init performs most actions in an idempotent way and can be used to ensure registration and/or software manipulation.

* Use the cluster-deploy.sh script in the terraform subdirectory to deploy fully functioning CaaSP clusters to KVM hosts, one cluster per host
** The script gives guidance on how to specify which KVM hosts to target
** An additional global cluster worker node is automatically deployed on each targeted host
*** Specifying 1 megabyte of memory for the global cluster worker nodes will effectively disable them
* Use the cluster-destroy.sh script in the same way to fully remove clusters from targeted hosts
* After a minimum of two global cluster worker nodes have been deployed they can be used to form a CaaSP cluster with one cluster node per KVM host
** The global cluster nodes can be used as worker nodes, master nodes, or even an admin node
** The minimum viable CaaSP cluster includes one admin node (which can be shared with other CaaSP clusters), one master node, and two worker nodes

=== Todo:

* *DONE* Create key pair for the admin node to use in the deployment
** *CONCERN* Biggest problem is getting the public key for the student host, to populate each cluster nodes' authorized_keys file 
** Instructor host needs to have the public keys of all student hosts
*** The authorized keys file for each deployment will contain:
**** Instructor host
**** Student host
**** Admin node

* *DONE* Copy .all_nodes and ssh_config files to the admin node

* *DONE* Remove load balancer

* Notes from adding local container registry as default registry:

* Install docker registry on admin node, from: https://documentation.suse.com/sles/15-SP1/single-html/SLES-dockerquick/#sec-docker-registry-definition

----
sudo SUSEConnect --product PackageHub/15.1/x86_64

sudo zypper refresh

sudo zypper install docker-distribution-registry
sudo systemctl enable registry
sudo systemctl start registry
----

** Registry will be available at caasp-admin:5000

* Install podman: `sudo zypper -n install podman`

* Update /etc/containers/registries.conf with:

----
[registries.insecure]
registries = ["caasp-admin:5000"]
----

* Pull all needed container images from external registry
** To find the container images needed for pods that are currently running on the base CaaSP cluser (repeat for all nodes in the cluster to see images needed for all running containers):
----
for EACH in $(kgpa | grep master-0 | awk '{print$2}'); do kubectl describe pod -n kube-system $EACH | grep Image | grep -v sha256 >> /tmp/images; done
for EACH in $(kgpa | grep worker-0 | awk '{print$2}'); do kubectl describe pod -n kube-system $EACH | grep Image | grep -v sha256 >> /tmp/images; done
for EACH in $(awk '{print$2}' /tmp/images | sort | uniq); do sudo podman pull $EACH; done
----

* Retag and push all images to admin local registry

----
for EACH in $(sudo podman images | grep suse  | grep caasp | grep -v REPOSITORY | awk '{print$1":"$2}'); do LOCAL=$(echo $EACH | awk -F/ '{print$4}'); sudo podman tag $EACH caasp-admin:5000/$LOCAL; sudo podman push caasp-admin:5000/$LOCAL; done
----

** Images that were running on the base CaaSP cluster at the time of this writing were:
----
registry.suse.com/caasp/v4/helm-tiller:2.16.1
registry.suse.com/caasp/v4/kured:1.2.0-rev4
registry.suse.com/caasp/v4/etcd:3.3.15
registry.suse.com/caasp/v4/hyperkube:v1.16.2
registry.suse.com/caasp/v4/coredns:1.6.2
registry.suse.com/caasp/v4/gangway:3.1.0-rev4
registry.suse.com/caasp/v4/cilium:1.5.3
registry.suse.com/caasp/v4/cilium-operator:1.5.3
registry.suse.com/caasp/v4/cilium-init:1.5.3
registry.suse.com/caasp/v4/caasp-dex:2.16.0
----

////
* Test deploying onto multiple KVM hosts
** Run ssh-agent and ensure you have passwordless ssh and sudo on the target host
----
terraform apply -state=state/infra1.tfstate -var libvirt_uri="qemu+ssh://admin@infra1.susecon.local/system"
terraform apply -state=state/infra2.tfstate -var libvirt_uri="qemu+ssh://admin@infra2.susecon.local/system"
terraform apply -state=state/infra3.tfstate -var libvirt_uri="qemu+ssh://admin@infra3.susecon.local/system"
terraform apply -state=state/infra4.tfstate -var libvirt_uri="qemu+ssh://admin@infra4.susecon.local/system"
----


*  Love to be able to do nested deployments

* Deploy a single deployment across multiple KVM hosts (providers)
** https://www.terraform.io/docs/configuration/providers.html#alias-multiple-provider-instances

* Need to work out a way to snapshot all environments after they've been deployed
** Also need a programatic way to rollback one or all environments
////


==== NFS storage class
* From: https://documentation.suse.com/suse-caasp/4.1/single-html/caasp-admin/#helm_tiller_install

----
you only need to run the following command from the location where you normally run skuba commands:

sudo zypper install helm

This will install Tiller without additional certificate security.

kubectl create serviceaccount --namespace kube-system tiller

kubectl create clusterrolebinding tiller \
    --clusterrole=cluster-admin \
    --serviceaccount=kube-system:tiller

helm init \
    --tiller-image registry.suse.com/caasp/v4/helm-tiller:2.16.1 \
    --service-account tiller

----



// vim: set syntax=asciidoc:

