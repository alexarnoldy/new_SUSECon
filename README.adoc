=== Requirements:

* At least one KVM host with enough memory and CPU capacity to support the deployed cluster nodes
* Each KVM host must have a linux bridge that has access to a LAN
** The LAN can be non-routable, if so desired
* An RMT server, with the CaaSP registration code and mirrored repositories, that is accessible from each KVM server
* Each KVM host should have access the Internet or a local container and Helm registry server

=== Operation:

IMPORTANT: This methodology requires that the KVM servers have a specific naming convention. It requires that they all have the same prefix, i.e. "infra" and then use a numbered suffix, ideally in sequential order. The testing environment this project was developed on had four KVM hosts named infra1, infra2, infra3, infra4. There is a small amount of flexibility in the naming scheme but any deviation from this scheme requires altering the variables.tf file to match.

IMPORTANT: This project is based on the SLES JeOS OpenStack Cloud image, from https://download.suse.com/index.jsp, which has cloud init pre-configured and ready for first boot.

* Download the latest SLES JeOS OpenStack qcow2 image into the terraform subdirectory (below where this README exists)
** If possible, choose the most recent QU* (Quarterly Update) version of the image
* Update the terraform/variables.tf file to suit your environment
** Depending on how closely your KVM hosts match the expected naming convention, the minimum changes to the variables.tf file that are required might be the number of master nodes, worker nodes, and the amount of CPU and memory resources allocated to each node.

TIP: Review the cloud init files in the cloud-init and global-cluster-cloud-init subdirectories to ensure they are performing the actions you are expecting. Some of the registration and software install/update functions may be commented out to speed deployment (which requires that the qcow2 image be registered/updated/configured ahead of time.) When in doubt, cloud init performs most actions in an idempotent way and can be used to ensure registration and/or software manipulation.

* Use the cluster-deploy.sh script in the terraform subdirectory to deploy fully functioning CaaSP clusters to KVM hosts, one cluster per host
** The script gives guidance on how to specify which KVM hosts to target
** An additional global cluster worker node is automatically deployed on each targeted host
*** Specifying 1 megabyte of memory for the global cluster worker nodes will effectively disable them
* Use the cluster-destroy.sh script in the same way to fully remove clusters from targeted hosts
* After a minimum of two global cluster worker nodes have been deployed they can be used to form a CaaSP cluster with one cluster node per KVM host
** The global cluster nodes can be used as worker nodes, master nodes, or even an admin node
** The minimum viable CaaSP cluster includes one admin node (which can be shared with other CaaSP clusters), one master node, and two worker nodes

=== Todo:

* *DONE* Create key pair for the admin node to use in the deployment
** *CONCERN* Biggest problem is getting the public key for the student host, to populate each cluster nodes' authorized_keys file 
** Instructor host needs to have the public keys of all student hosts
*** The authorized keys file for each deployment will contain:
**** Instructor host
**** Student host
**** Admin node

* *DONE* Copy .all_nodes and ssh_config files to the admin node

* *DONE* Remove load balancer

== Creating and populating a local container registry, which may be mounted as a separate partition

NOTE: The first part of the procedure can be used for most situations, while the second part is specific to populating a registry that exists on a qcow2 image that has been mounted to this VM or host.

* Install docker registry on admin node, from: https://documentation.suse.com/sles/15-SP1/single-html/SLES-dockerquick/#sec-docker-registry-definition

----
sudo SUSEConnect --product PackageHub/15.1/x86_64

sudo zypper refresh

sudo zypper install docker-distribution-registry
sudo systemctl enable registry
sudo systemctl start registry
----

* If the local registry is in a qcow2 image mounted to this Update Host, install the registry software into that location:

----
export MOUNTPOINT=/mnt/
sudo SUSEConnect --root ${MOUNTPOINT} --product PackageHub/15.1/x86_64
sudo zypper --root ${MOUNTPOINT} refresh
sudo zypper --root ${MOUNTPOINT} install docker-distribution-registry
----

** Registry will be available at $(hostname):5000

* Install podman: `sudo zypper -n install podman`

* Update /etc/containers/registries.conf with:
** Comment out all existing entries (which are in a v1 format)
** Add these entries:
----
[registries.insecure]
registries = ["caasp-admin:5000"]
----

* Pull all needed container images from external registries
** The container images needed for pods that are currently running on the base CaaSP cluster (repeat for all nodes in the cluster to see images needed for all running containers) can be found with these loops:
----
for EACH in $(kgpa | grep master-0 | awk '{print$2}'); do kubectl describe pod -n kube-system $EACH | grep Image | grep -v sha256 >> /tmp/images; done
for EACH in $(kgpa | grep worker-0 | awk '{print$2}'); do kubectl describe pod -n kube-system $EACH | grep Image | grep -v sha256 >> /tmp/images; done
----

** Images that were running on the base CaaSP cluster at the time of this writing were:
----
registry.suse.com/caasp/v4/helm-tiller:2.16.1
registry.suse.com/caasp/v4/kured:1.2.0-rev4
registry.suse.com/caasp/v4/etcd:3.3.15
registry.suse.com/caasp/v4/hyperkube:v1.16.2
registry.suse.com/caasp/v4/coredns:1.6.2
registry.suse.com/caasp/v4/gangway:3.1.0-rev4
registry.suse.com/caasp/v4/cilium:1.5.3
registry.suse.com/caasp/v4/cilium-operator:1.5.3
registry.suse.com/caasp/v4/cilium-init:1.5.3
registry.suse.com/caasp/v4/caasp-dex:2.16.0
registry.suse.com/caasp/v4/pause:3.1
----
*** NOTE: The pause container doesn't show up, but is needed

*** In addition, the nfs-client-provisioner was running in the default namespace:
----
quay.io/external_storage/nfs-client-provisioner:v3.1.0-k8s1.11
----

* Once the list of containers is established, pull them down to this VM or host:
----
for EACH in $(awk '{print$2}' /tmp/images | sort | uniq); do sudo podman pull $EACH; done
----

* Update the docker registry `/etc/registry/config.yml` config file on the Update Platform:
** Change `rootdirectory:` to point to `/var/lib/docker-registry` at the qcow2 image mount point

* Retag and push all images to admin local registry

----
export HOSTNAME=$(hostname)
export MOUNTPOINT=/mnt/
## The container images have to be pushed into the location they identify as the registry in their manifests
export REGISTRY=registry.suse.com
mkdir -p ${MOUNTPOINT}/var/lib/docker-registry/docker/registry/v2/repositories/${REGISTRY}

for EACH in $(sudo podman images | grep suse  | grep caasp | grep -v REPOSITORY | awk '{print$1":"$2}'); do LOCAL=$(echo $EACH | awk -F/ '{print $(NF)}'); sudo podman tag $EACH ${HOSTNAME}:5000/${REGISTRY}/$LOCAL; sudo podman push ${HOSTNAME}:5000/${REGISTRY}/$LOCAL; done

## This is for the nfs-client-provisioner, which has a manifest that identifies the registry as quay.io/external_storage/
export MOUNTPOINT=/mnt/
export REGISTRY=quay.io/external_storage
mkdir -p ${MOUNTPOINT}/var/lib/docker-registry/docker/registry/v2/repositories/${REGISTRY}

for EACH in $(sudo podman images | grep nfs-client-provisioner | grep -v REPOSITORY | awk '{print$1":"$2}'); do LOCAL=$(echo $EACH | awk -F/ '{print $(NF)}'); sudo podman tag $EACH ${HOSTNAME}:5000/${REGISTRY}/$LOCAL; sudo podman push ${HOSTNAME}:5000/${REGISTRY}/$LOCAL; done
----

////
* Update /etc/containers/registries.conf with:

** Comment out: `registries = ["docker.io"]`
** Add:
----
[registries.search]
registries = ["caasp-admin:5000"]
----

==== Populating a registry while the qcow2 image is mounted to a running VM or host Update Platform

* Install podman on the VM or host Update Platform
* Install the docker registry on the Update Platform
* Install the docker registry into the qcow2 
////

////
* create the `/etc/systemd/system/multi-user.target.wants/registry.service â†’ /usr/lib/systemd/system/registry.service` symlink in the qcow2 image:
----
cd etc/systemd/system/multi-user.target.wants/
ln -s ../../../../usr/lib/systemd/system/registry.service registry.service
----
* Update the docker registry `/etc/registry/config.yml` config file on the Update Platform:
** Change `rootdirectory:` to point to `/var/lib/docker-registry` on the qcow2 image
** See if port 5000 is open: `sudo ss -npr --listening`
* Update /etc/containers/registries.conf with:
----
[registries.insecure]
registries = ["<hostname>:5000"]
----
** Replace <hostname> with the hostname of the Update Platform

* Restart the docker registry: `sudo systemctl restart registry`
* Use the sections above to pull the `registry.suse.com/caasp/v4/*` and `nfs-client-provisioner` images, then push them to the local registry
////
* Verify the images are in the repository on the qcow2 images:
----
export MOUNTPOINT=""
ls ${MOUNTPOINT}//var/lib/docker-registry/docker/registry/v2/repositories/
----
* 

==== Updating the qcow2 image that is mounted to a running VM or host Update Platform to use the local registry (on the admin node)

* Update /${MOUNTPOINT}/etc/containers/registries.conf with:

** Comment out all existing entries (which are in a v1 format)
** Add these entries:

----
[[registry]]
blocked = false
insecure = true
prefix = "registry.suse.com"
location = "caasp-admin:5000/registry.suse.com"

[[registry]]
blocked = false
insecure = true
prefix = "quay.io"
location = "caasp-admin:5000/quay.io"
----


////
=== Updating CaaSP nodes to use local registry

* Can try to set up the /etc/containers/registries.conf file in the image
* Should also put it in the files directory and add a comment in the cloud-init files about adding it, if needed
* Need to see if it exists after installing the CaaSP Node pattern
////
////
* Test deploying onto multiple KVM hosts
** Run ssh-agent and ensure you have passwordless ssh and sudo on the target host
----
terraform apply -state=state/infra1.tfstate -var libvirt_uri="qemu+ssh://admin@infra1.susecon.local/system"
terraform apply -state=state/infra2.tfstate -var libvirt_uri="qemu+ssh://admin@infra2.susecon.local/system"
terraform apply -state=state/infra3.tfstate -var libvirt_uri="qemu+ssh://admin@infra3.susecon.local/system"
terraform apply -state=state/infra4.tfstate -var libvirt_uri="qemu+ssh://admin@infra4.susecon.local/system"
----


*  Love to be able to do nested deployments

* Deploy a single deployment across multiple KVM hosts (providers)
** https://www.terraform.io/docs/configuration/providers.html#alias-multiple-provider-instances

* Need to work out a way to snapshot all environments after they've been deployed
** Also need a programatic way to rollback one or all environments
////


==== NFS storage class
* From: https://documentation.suse.com/suse-caasp/4.1/single-html/caasp-admin/#helm_tiller_install

----
you only need to run the following command from the location where you normally run skuba commands:

sudo zypper install helm

This will install Tiller without additional certificate security.

kubectl create serviceaccount --namespace kube-system tiller

kubectl create clusterrolebinding tiller \
    --clusterrole=cluster-admin \
    --serviceaccount=kube-system:tiller

helm init \
    --tiller-image registry.suse.com/caasp/v4/helm-tiller:2.16.1 \
    --service-account tiller

----



// vim: set syntax=asciidoc:

